#!/usr/bin/env python3
"""
Dataset Download Script for RunPod

This script downloads and prepares the LibriSpeech dataset directly on RunPod,
avoiding the need to commit large audio files to Git.
"""

import os
import sys
import logging
import requests
import tarfile
from pathlib import Path
from tqdm import tqdm
import hashlib

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Dataset configuration
DATASET_CONFIG = {
    'librispeech_dev_clean': {
        'url': 'https://www.openslr.org/resources/12/dev-clean.tar.gz',
        'filename': 'dev-clean.tar.gz',
        'expected_size_mb': 322,  # Updated to current size (~322 MB)
        'md5_hash': '42e2234ba48799c1f50f24a7926300a1',  # Updated hash from your download
        'extract_path': 'LibriSpeech',
        'target_path': 'data/realtime_dataset/LibriSpeech/dev-clean'
    }
}

def download_file(url, filename, expected_size_mb=None):
    """Download a file with progress bar and size verification"""
    logger.info(f"üì• Downloading {filename} from {url}")
    
    # Create target directory (only if filename has a path)
    dirname = os.path.dirname(filename)
    if dirname:  # Only create directory if there's a path component
        os.makedirs(dirname, exist_ok=True)
    
    # Download with progress bar
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))
    
    with open(filename, 'wb') as file, tqdm(
        desc=filename,
        total=total_size,
        unit='iB',
        unit_scale=True,
        unit_divisor=1024,
    ) as pbar:
        for data in response.iter_content(chunk_size=1024):
            size = file.write(data)
            pbar.update(size)
    
    # Verify file size
    actual_size_mb = os.path.getsize(filename) / (1024 * 1024)
    logger.info(f"‚úÖ Downloaded: {filename} ({actual_size_mb:.1f} MB)")
    
    if expected_size_mb and abs(actual_size_mb - expected_size_mb) > 10:
        logger.warning(f"‚ö†Ô∏è  File size differs from expected: {actual_size_mb:.1f} MB vs {expected_size_mb} MB")
    
    return filename

def verify_md5(filename, expected_hash):
    """Verify file integrity using MD5 hash"""
    logger.info(f"üîç Verifying file integrity...")
    
    hash_md5 = hashlib.md5()
    with open(filename, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    
    actual_hash = hash_md5.hexdigest()
    
    if actual_hash == expected_hash:
        logger.info("‚úÖ MD5 hash verification passed")
        return True
    else:
        logger.warning(f"‚ö†Ô∏è  MD5 hash verification failed!")
        logger.warning(f"   Expected: {expected_hash}")
        logger.warning(f"   Actual:   {actual_hash}")
        logger.warning(f"   Dataset may have been updated. Continuing...")
        return False

def update_hash_if_needed(filename, expected_hash):
    """Update the hash if verification fails - useful for dataset updates"""
    hash_md5 = hashlib.md5()
    with open(filename, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    
    actual_hash = hash_md5.hexdigest()
    
    if actual_hash != expected_hash:
        logger.info(f"üí° New dataset hash detected: {actual_hash}")
        logger.info(f"   You can update the script with this hash for future runs")
        return actual_hash
    
    return expected_hash

def extract_tar(filename, extract_path):
    """Extract tar.gz file with progress"""
    logger.info(f"üì¶ Extracting {filename} to {extract_path}")
    
    # Create extract directory
    os.makedirs(extract_path, exist_ok=True)
    
    # Extract with progress
    with tarfile.open(filename, 'r:gz') as tar:
        members = tar.getmembers()
        for member in tqdm(members, desc="Extracting"):
            tar.extract(member, extract_path)
    
    logger.info(f"‚úÖ Extraction complete: {extract_path}")

def setup_dataset_structure():
    """Create the proper dataset directory structure"""
    logger.info("üèóÔ∏è  Setting up dataset directory structure...")
    
    # Create directories with relative paths
    directories = [
        'data/realtime_dataset',
        'data/realtime_dataset/LibriSpeech',
        'data/realtime_dataset/LibriSpeech/dev-clean',
        'checkpoints/optimized',
        'results',
        'logs'
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        logger.info(f"   ‚úÖ Created: {directory}")

def create_manifest_from_downloaded_data():
    """Create manifest.json from the downloaded LibriSpeech data"""
    logger.info("üìù Creating manifest from downloaded data...")
    
    # Use relative paths for portability
    librispeech_path = Path("data/realtime_dataset/LibriSpeech/dev-clean")
    manifest_path = Path("data/realtime_dataset/manifest.json")
    
    if not librispeech_path.exists():
        logger.error("‚ùå LibriSpeech data not found. Please download first.")
        return False
    
    # Find all audio files - account for nested structure
    audio_files = list(librispeech_path.rglob("*.flac"))
    logger.info(f"   Found {len(audio_files)} audio files")
    
    if len(audio_files) == 0:
        logger.error("‚ùå No audio files found in downloaded data")
        return False
    
    # Create manifest entries
    manifest_entries = []
    for audio_file in audio_files:
        # Extract speaker and chapter info from path
        parts = audio_file.parts
        if len(parts) >= 6:  # Account for nested structure
            speaker_id = parts[-3]  # speaker_id
            chapter_id = parts[-2]  # chapter_id
            filename = parts[-1]    # filename.flac
            
            # Calculate relative path from LibriSpeech root (account for nested structure)
            # The path should be relative to data/realtime_dataset/LibriSpeech/dev-clean
            relative_path = str(audio_file.relative_to(Path("data/realtime_dataset/LibriSpeech/dev-clean")))
            
            entry = {
                "audio_path": relative_path,  # Changed from "audio_file" to match simple_loader.py
                "speaker": speaker_id,        # Changed from "speaker_id" to match simple_loader.py
                "chapter_id": chapter_id,
                "filename": filename,
                "duration": None,  # Will be calculated during training
                "split": "train" if "dev-clean" in str(audio_file) else "val"
            }
            manifest_entries.append(entry)
    
    # Split into train/val (80/20)
    import random
    random.shuffle(manifest_entries)
    
    split_point = int(len(manifest_entries) * 0.8)
    train_entries = manifest_entries[:split_point]
    val_entries = manifest_entries[split_point:]
    
    # Update split labels
    for entry in train_entries:
        entry["split"] = "train"
    for entry in val_entries:
        entry["split"] = "val"
    
    # Create manifest
    manifest = {
        "dataset_name": "LibriSpeech-dev-clean",
        "total_files": len(manifest_entries),
        "train_files": len(train_entries),
        "val_files": len(val_entries),
        "sample_rate": 16000,
        "audio_format": "flac",
        "entries": manifest_entries
    }
    
    # Save manifest
    import json
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)
    
    logger.info(f"‚úÖ Manifest created: {manifest_path}")
    logger.info(f"   Train files: {len(train_entries)}")
    logger.info(f"   Val files: {len(val_entries)}")
    
    return True

def cleanup_download_files():
    """Clean up temporary download files"""
    logger.info("üßπ Cleaning up temporary files...")
    
    files_to_remove = [
        'dev-clean.tar.gz',
        'LibriSpeech'  # Remove extracted directory after moving
    ]
    
    for file_path in files_to_remove:
        if os.path.exists(file_path):
            if os.path.isdir(file_path):
                import shutil
                shutil.rmtree(file_path)
            else:
                os.remove(file_path)
            logger.info(f"   ‚úÖ Removed: {file_path}")

def main():
    """Main function to download and setup dataset"""
    logger.info("üöÄ RunPod Dataset Setup - LibriSpeech")
    logger.info("="*50)
    
    try:
        # 1. Setup directory structure
        setup_dataset_structure()
        
        # 2. Download dataset
        config = DATASET_CONFIG['librispeech_dev_clean']
        filename = download_file(
            config['url'], 
            config['filename'],
            config['expected_size_mb']
        )
        
        # 3. Verify integrity
        if not verify_md5(filename, config['md5_hash']):
            logger.warning("‚ö†Ô∏è  MD5 hash verification failed!")
            logger.warning("   This may indicate the dataset has been updated.")
            # Detect the new hash for future reference
            new_hash = update_hash_if_needed(filename, config['md5_hash'])
            logger.warning("   Continuing with dataset setup...")
            # Don't return False - continue with the setup
        else:
            logger.info("‚úÖ File integrity verified")
        
        # 4. Extract dataset
        extract_tar(filename, config['extract_path'])
        
        # 5. Move to proper location
        import shutil
        target_path = Path(config['target_path'])
        if target_path.exists():
            shutil.rmtree(target_path)
        
        shutil.move(config['extract_path'], target_path)
        logger.info(f"‚úÖ Dataset moved to: {target_path}")
        
        # 6. Create manifest
        if not create_manifest_from_downloaded_data():
            return False
        
        # 7. Cleanup
        cleanup_download_files()
        
        logger.info("\nüéâ Dataset setup complete!")
        logger.info(f"üìä Dataset location: {config['target_path']}")
        logger.info(f"üìù Manifest: data/realtime_dataset/manifest.json")
        logger.info(f"üöÄ Ready for training!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Dataset setup failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1) 